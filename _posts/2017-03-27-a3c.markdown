---
layout: post
title:  "Reinforcement learning with the A3C algorithm"
date:   2017-03-27 17:00:00 +0100
categories: reinforcement-learning
---

$
\def\RR{\mathbb{R}}
\def\tr{\mathrm{tr}}
\def\th{\mathrm{th}}
\def\EE{\mathbb{E}}
\def\coloneqq{\colon=}
\def\grad{\nabla}
$

# Introduction #
I've been playing around with deep reinforcement learning for a little while,
but have always found it hard to get the state of the art algorithms working.
This is in part because getting any algorithm to work requires some good choices
for hyperparameters, and I have to do all of these experiments on my Macbook.

In this tutorial I will go over how to implement the asynchronous advantage
actor-critic algorithm (or A3C for short). The original paper can be found
[here](https://arxiv.org/abs/1602.01783) but I hope that I can contribute by
making everything a little easier to understand. The paper describes 4
algorithms: one step Q-learning, $n$-step Q-learning, one step SARSA and A3C.
I also implemented one step Q-learning and got this to work on Space Invaders,
but the reason I focus on A3C is because it is the best performing algorithm
from the paper.

The exciting thing about the paper, at least for me, is that you don't need to
rely on a GPU for speed. In fact, the whole idea is to use multiple cores of a
CPU, run in parallel, which gives a speedup proportional to the number of cores
used. Since I normally run things on my laptop, which only has one CPU (albeit
with two cores), I don't actually bother implementing things properly in
parallel, and instead use threads. Consequently, my implementation won't
actually give a speedup in this way, but we'll see that it still manages to
work on a laptop, just a little more slowly.

I'll use tensorflow to make things a little easier, as we'll need to work with
convolutional neural networks, but in principle everything could be implemented
using just numpy if you were willing to write the backprop code yourself.

My aim is to make the algorithm easy to understand, and also make it so that
there are as few choices of hyperparameters and as little deep learning magic as
possible.

# The A3C algorithm #
As with a lot of recent progress in deep reinforcement learning, the innovations
in the paper weren't really dramatically new algorithms, but how to force
relatively well known algorithms to work well with a deep neural network. As I
will soon explain in more detail, the A3C algorithm can be essentially described
as using policy gradients with a function approximator, where the function
approximator is a deep neural network and the authors use a clever method to try
and ensure the agent explores the state space well.

In the paper there are actually two versions of the A3C algorithm: one just uses
a feedforward convolutional neural network, while the other includes a recurrent
layer. I'll focus on the first one, in order to simplify everything as much as
possible. I also only focus on the discrete action case here. Perhaps I will
write a follow up to this including the recurrent layer, as well as the
extension to continuous action spaces.

# Brief reminder of reinforcement learning #
I had an introduction to reinforcement learning and the policy gradient method
in my first post on reinforcement learning, so it might be worth reading that
first, but I will briefly summarise what we need here anyway.

First we need to discuss actions and states. I will give some examples using
ATARI games and then define them formally.

For example, in Space Invaders, the possible $a$ might be: 0 for moving left, 1
for moving right and 2 for shoot. And if we are playing ATARI games from pixel
values, then the input $x$ just looks like a screenshot of the game. This can be
represented as a 3D array by considering each of the red, green and blue colour
channels as being a 2D image.

Now, an ATARI game screen is $210 \times 160$ pixels, so we would have $210
\times 160 \times 3 = 100,800$ entries in the array representing one screen.  In
fact, one can see that a single game screen gives insufficient information to
the agent, since it doesn't show which direction objects are moving in.

In the following, we assume that the action space is some finite set of actions,
and we can thus label them $1, 2, \ldots, n$. We also assume that $x$ is an
$n$-dimensional vector, or more generally, an array, that represents the current
state. In fact, in the ATARI setup we don't use 1-dimensional arrays (i.e.
vectors), but instead use 3d arrays. The first and second dimensions are just
the width and height of the input image, and the third dimension is time. You
can think of these as just being a sequence of input images.

# Overview of policy gradients #

The idea of a policy is to parametrise the conditional probability of performing
a given action $a$ given some state $x$. That is, given a state $x$, we want to
compute, for each possible action $a$, the probability $P(a | x; \theta)$ of
performing action $a$ given that we are in state $x$. We write $\theta$ inside
the probability to denote that the probability is determined by the parameter
$\theta$. In the following $\theta$ will be the weights of our deep neural
network, so will actually be a million numbers or so.

I think there is sometimes confusion with what $P(a | x; \theta)$ means. For us,
it means that while the agent is playing the game, or training, whenever we see
state $x$ we will choose the action by first computing $P(a | x; \theta)$ for
each action $a$ and then sampling the action from this probability distribution.
As it happens, if you really have a Markov Decision Process (i.e. the future is
independent of the past, given the current state), then if an optimal policy
exists, there is a deterministic optimal policy. However, using a stochastic
policy (i.e. choosing actions with some randomness) is helpful. For one, it
allows the agent to explore the state space without always taking the same
trajectory, which is important for the exploration-exploitation tradeoff. In
Space Invaders, for example, it turns out that if you choose to shoot at each
timestep then you get a reward of something like 180 every time. If you weren't
forced to explore other actions, you may well think this was good enough and
exploit this (over, say, always moving left, which would give you zero reward).

To fit with standard notation, we define $\pi(a | x; \theta) = P(a | x;
\theta)$. We are now looking to find a good set of parameters $\theta$, such
that, if we follow the policy $\pi(a | x; \theta)$, then we have a high expected
reward.

Denote by $\tau$ a trajectory of the MDP. That is, $\tau$ is a sequence $x_0,
a_0, r_1, x_1, a_1, r_2, \ldots, x_{T-1}, a_{T-1}, r_T$, where $r_{t+1}$ is the
reward for having been in state $x_t$, taken action $a_t$, and being
transitioned to state $x_{t+1}$. In the episodic setting, we have terminal
states and we assume that the sequence terminates after some finite time $T$
(e.g.  when the agent dies or completes the game). We let $R_\tau = r_1 + \cdots + r_T$, or sometimes we use a discount factor $0 < \gamma \le 1$ and let $R_\tau
= r_1 + \gamma r_2 + \cdots \gamma^{T-1} r_T$.

The policy gradient method aims to maximise $\EE_\tau[R_\tau | \pi; \theta]$. In
words, this is the expected reward of a trajectory when the agent follows policy
$\pi$. The method tries to maximise this by gradient ascent with respect to the
parameters $\theta$. Thus we compute $\grad_\theta \EE_\tau[R_\tau | \pi;
\theta]$ and then iteratively improve $\theta$ by $\theta \mapsto \theta +
\alpha \cdot \grad_\theta \EE_\tau [R_\tau | \pi; \theta]$.

We saw last time that we can compute $\grad_\theta \EE_\tau[R_\tau | \pi;
\theta]$ as the expected value of

$$
\hat{g}(\tau) = R_\tau \grad_\theta \sum_{t=0}^{T-1} \log \pi (a_t | x_t;
\theta).
$$

So we fix an initial parameter $\theta$ and then iterate the following: sample a
trajectory $\tau$ from the environment using $\pi(a | x; \theta)$; compute
$\hat{g}(\tau)$; update $\theta$ by $\theta \mapsto \theta + \alpha
\hat{g}(\tau)$. None of the updates will actually be in the correct direction,
but if we update enough times then on average the step will be correct.

The only thing we have to be able to do is compute $\grad_\theta \log \pi(a | x;
\theta)$ for any given action $a$ and state $x$. This is where tensorflow comes
in, which is basically an automatic differentiation machine.

## Policy gradients with lower variance ##
Although $\hat{g}(\tau)$ is an unbiased estimator of the gradient of the
expected reward, it has high variance, which means that we take a lot of
steps in poor directions, even though on average the step will be in the correct
direction. The next helpful idea is to try and reduce the variance of our
estimate of the gradient.

One can replace $R_\tau$ in the expression for $\hat{g}(\tau)$ by $R_t =
\sum_{t'=t}^T r_{t'}$ and get the same expectation. This is because if $b_t$ is
a function of states and actions up to time $t$, then $\EE_{s_{t+1\colon\infty},
a_{t+1\colon\infty}}(b_t) = b_t$ and is thus a constant. This just says that the
expectation of $b_t$ over future states and actions is just $b_t$ (which sounds
obvious when you say it like that!). Now note that $\EE(\grad_\theta
\sum_{t=0}^{T-1} \log \pi (a_t | x_t; \theta)) = \grad_\theta \EE(1) = 0$.
The first equality follows by the same trick as last time, where we both
multiply and divide by $P(\tau | \theta)$ inside the expectation. When we now
include the $b_t$ (which is constant), we thus also get zero expectation. Hence
the following two expectations are equal

$$
\EE (\sum_{t=0}^{T-1} \grad_\theta \log \pi (a_t | x_t; \theta) (R_\tau - b_t))
= \EE (\sum_{t=0}^{T-1} \grad_\theta \log \pi (a_t | x_t; \theta) R_\tau).
$$

If we take $b_t = r_1 + \cdots + r_{t-1}$, then we arrive at the estimator

$$
\sum_{t=0}^{T-1} \grad_\theta \log \pi (a_t | x_t; \theta) R_t
$$

of the gradient.

## A3C on the Cart and Pole problem ##
There's a really nice discussion of things you can replace $R_t$ with in this
[paper](https://arxiv.org/abs/1506.02438). The authors there investigate using
biased estimators instead of just the unbiased estimators that we have been
talking about. That is, the expected value of the estimator doesn't actually
agree with the gradient $\grad_\theta \EE_\tau (R_\tau)$, but they show that you
can get lower variance estimators this way.

In fact, if you just use $R_t$ for the estimator, you recover the REINFORCE
algorithm. If you use $R_t - b_t$, where $b_t$ is some function of the state
$s_t$ (that you have to learn) it is possible to reduce the variance, as we
discussed above.

The A3C algorithm changes this estimator by replacing $R_t$ by something called
the advantage function.

### Value functions ###
To discuss the advantage function, we first have to define some useful value
functions. Firstly, the state-value function $V_\pi(s)$ is defined as the
expected sum of rewards when starting in state $s$ and following policy $\pi$.
Formally,

$$
    V_\pi(s) = \EE_\tau( R_\tau | s_0 = s, \pi ).
$$

Even more formally, let $s_0 = s$ and $s_t, a_t$ be generated by the following
stochastic process:

$$
    a_t \sim \pi(a_t \mid s_t), s_{t+1} \sim P(s_{t+1} \mid s_t, a_t)
$$

for all $t \ge 0$. Then $R_\tau = r_1 + \cdots + r_T$ as usual, where $r_t$ is
the reward received by the agent for the sequence $s_t, a_t, s_{t+1}$.

Next we define the closely related state-action value function $Q_\pi(s, a)$.
This is the expected sum of rewards when starting in state $s$, taking action
$a$ and from then on following the policy $\pi$. We have

$$
    Q_\pi(s, a) = \EE_\tau( R_\tau | s_0 = s, a_0 = a, a_t \sim \pi ).
$$

More formally, let $s_0 = s, a_0 = a$, and let $s_t, a_t$ be generated by the
following stochastic process (for $t \ge 0$)

$$
    s_{t+1} \sim P(s_{t+1} \mid s_t, a_t), a_{t+1} \sim \pi(a_{t+1} \mid s_{t+1}).
$$

Then as before $R_\tau = r_1 + \cdots + r_T$, where $T$ is the length of the
episode (assumed finite). And $Q_\pi(s, a) = \EE_\tau(R_\tau)$ in the stochastic
process just described.

One can then see that $Q_\pi$ and $V_\pi$ satisfy the following equation:

$$
    Q_\pi(s, a) = \EE(r_1 | s_0 = s, a_0 = a) + \EE_{s_1} (V_\pi(s_1)).
$$

In words: the left hand side is the expected total reward when starting in state
$s$, taking action $a$ and then following policy $\pi$ for the rest of the
episode. The right hand side is the expected first reward for starting in state
$s$ plus the expected value (computed over all possible states $s_1$) of being
in state $s_1$ when following $\pi$.

One then defines the advantage function as $A_\pi(s, a) = Q_\pi(s, a) -
V_\pi(s)$. Intuitively, this is the advantage of taking action $a$ over
following the policy $\pi$ at this time step.

One could try and estimate the value function $b_t(s_t) \approx V_\pi(s_t)$, and
then note that $R_t$ is an estimate of $Q_\pi(s_t, a_t)$ and $b_t$ is an
estimate of $V_\pi(s_t)$, so that $R_t - b_t(s_t)$ is an estimate of $A_\pi(s_t,
a_t)$. This leads us to consider the estimator

$$
    \sum_{t=0}^{T-1} \grad_\theta \log \pi(a_t \mid s_t; \theta) (R_t -
    b_t(s_t)),
$$
where $b_t(s_t)$ is an estimate of $V_\pi(s_t)$, which we learn simultaneously.

## Advantage actor-critic ##

This leads us to the following algorithm, which could be called advantage
actor-critic (i.e. no 'asynchronous' yet). The critic refers to the estimate of
the value function, while the actor refers to the estimate of the policy.

~~~~pseudocode
initialise parameters theta_pi for policy pi
initialise parameters theta_v for value estimate V
choose max_t (e.g. 20)
choose discount factor gamma

# Update parameters once each time through this loop
for T in range(max_T):
    s = current state of emulator
    initialise array rewards
    for t in range(max_t):
        sample action a from pi(a | s; theta_pi)
        append a to actions
        append s to states
        perform action a and get new state s' and reward r
        append r to rewards
        s' = s
        break if terminal
    # Now train the parameters
    R = 0
    if not terminal
        R = V(s, theta_v) (the estimate of the latest state)
    # Compute discounted rewards
    append R to rewards array
    rewards = discounted(rewards, gamma)
    # Compute the gradients
    for each i
        R_i = sum(rewards[i:])
        val_i = V(s_i, theta_v)
        compute grad_theta_pi log pi(a_i, s_i, theta_pi) * (R_i - val_i)
        compute grad_theta_v (R_i - val_i)^2
        
    update theta_pi, theta_v by the computed gradients
~~~~

## Asynchronous advantage actor-critic ##

The above algorithm has the problem that the agent only observes one part of
state space at a time and so the updates might only affect the agent's
performance positively in these parts of state space while decreasing the
agent's performance in other areas. Also, since the updates come from
consecutive states played by the agent, the updates are correlated, which is
usually a bad thing in machine learning. Even in supervised learning, (for
example, classifying images), it is good practice to shuffle training examples
while training so as to avoid correlated updates to the network. In the
reinforcement learning setting, it can be disastrous!

In the DQN (deep Q-learning) algorithm, the authors get around this problem of
correlated updates by using experience replay: a large buffer of all transitions
$s_t, a_t, s_{t+1}, r_{t+1}$ that the agent continually adds to. During
training, you sample from this replay buffer randomly and use these samples to
make updates. This ensures you see uncorrelated transitions, as opposed to
transitions in sequence.

With the A3C algorithm, we no longer use experience replay, but instead just use
many agents, all exploring the state space simultaneously. The hope is that the
different agents will be in different parts of the state space, and thus give
uncorrelated updates to the gradients.

Implemented as stated in the asynchronous paper, each agent should run in a
separate process so that the training occurs in parallel. A central network
holds shared parameters `theta_pi, theta_v` and the agents asynchronously update
to these. This just means that the updates are not synchronised, i.e. each agent
updates the shared parameters as soon as it can. Between each update, the agent
takes a copy of the shared network, with parameters `theta_local_pi,
theta_local_v`, say, and then runs `t_max` steps of the simulator, acting via
the local policy. After `t_max` steps, or if it sees a terminal frame, then that
agent computes the gradients (in its own process) and then updates the shared
parameters. If this all works in parallel, you expect to see a linear speedup
with the number of agents.

## Tricks to get it to work ##

This part is coming soon!

While you wait... Here is the agent playing Space Invaders:

<iframe width="560" height="315" src="https://www.youtube.com/embed/xXP77QiHFTs?autoplay=1&amp;rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
